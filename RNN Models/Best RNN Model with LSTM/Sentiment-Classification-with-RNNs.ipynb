{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Using RNNs \n",
    "\n",
    "### In this example, we see sentiment classification.\n",
    "### Intead of an Elman RNN built using the RNNCell, use built-in RNNs: nn.RNN, nn.LSTM, and nn.GRU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Urls and HTML links\n",
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "# Lower casing\n",
    "def lower(text):\n",
    "    low_text= text.lower()\n",
    "    return low_text\n",
    "\n",
    "# Number removal\n",
    "def remove_num(text):\n",
    "    remove= re.sub(r'\\d+', '', text)\n",
    "    return remove\n",
    "\n",
    "#Remove stopwords & Punctuations\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "# STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS = ['i', 'me', 'my', 'myself','i', 'we', 'our', 'ours', 'ourselves','we', 'you', 'your', 'yours',\n",
    "             'yourself', 'yourselves', 'he', 'him','his', 'himself', 'she', 'her', 'hers','herself', \n",
    "             'it', 'its', 'itself','they', 'them', 'their','theirs', 'themselves', 'what','which', 'who', 'whom',\n",
    "             'this', 'that', 'these','those', 'am', 'is','are', 'was', 'were','be', 'been', 'being',\n",
    "             'it', 'your', 'yours','it', 'your', 'yours','it', 'your', 'yours','it', 'your', 'yours',\n",
    "             'have', 'has', 'had','having', 'do', 'does','did', 'doing', 'a','an', 'the','and', 'but', 'if',\n",
    "             'or', 'because','as', 'until', 'while','of', 'at','by', 'for', 'with','about', 'against','between',\n",
    "             'into', 'through','during', 'before','after', 'above', 'below','to']\n",
    "            \n",
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Remove mentions and hashtags\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "\n",
    "#Remove extra white space left while removing stuff\n",
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return space_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary, Vectorizer, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"   \n",
    "    def __init__(self, review_vocab, rating_vocab, max_sequnce_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            rating_vocab (Vocabulary): maps class labels to integers; {'negative':0, 'positive':1}\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "        self.max_sequnce_length = max_sequnce_length\n",
    "\n",
    "    def vectorize(self, review, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review (str): the string of words\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "        indices = [self.review_vocab.begin_seq_index]       \n",
    "                \n",
    "        for token in str(review).split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                indices.append(self.review_vocab.lookup_token(token))\n",
    "\n",
    "        indices.append(self.review_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)         \n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.review_vocab.mask_index\n",
    "        \n",
    "        return out_vector, len(indices)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=2):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = SequenceVocabulary()\n",
    "        rating_vocab = Vocabulary()\n",
    "        \n",
    "        max_sequnce_length = 0\n",
    "        \n",
    "        # Add ratings\n",
    "        for rating in sorted(set(review_df.Sentiment)):\n",
    "            rating_vocab.add_token(rating)\n",
    "\n",
    "        \n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.OriginalTweet:\n",
    "            tokens = str(review).split(\" \")\n",
    "            if len(tokens) > max_sequnce_length:\n",
    "                max_sequnce_length = len(tokens) \n",
    "            for word in tokens:\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "\n",
    "        return cls(review_vocab, rating_vocab, max_sequnce_length)   \n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        review_vocab = SequenceVocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab =  Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "\n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(), \n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df \n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self._max_seq_length = self._vectorizer.max_sequnce_length + 2   # add 2 for begin_seq_token and end_seq_token\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                             'val': (self.val_df, self.validation_size), \n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = self.train_df.Sentiment.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.rating_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:lower(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_urls(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_html(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_num(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:punct_remove(x))\n",
    "#         review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_stopwords(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_mention(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_hash(x))\n",
    "        review_df['OriginalTweet']=review_df['OriginalTweet'].apply(lambda x:remove_space(x))\n",
    "        \n",
    "        train_review_df = review_df[review_df.split=='train']\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df))\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, review_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            review_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(review_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of ReviewVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return ReviewVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's:\n",
    "                features (x_data)\n",
    "                label (y_target)\n",
    "                feature length (x_length)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector, vec_length = \\\n",
    "            self._vectorizer.vectorize(row.OriginalTweet, self._max_seq_length)\n",
    "        \n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.Sentiment)\n",
    "\n",
    "        return {'x_data': review_vector, \n",
    "                'y_target': rating_index, \n",
    "                'x_length': vec_length}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=False, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index.\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "def column_summation(y_out, x_lengths):\n",
    "    '''Get a max or mean vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the max or mean vector of all the vectors by \n",
    "    the position indicated by the corresponding value in `x_lengths` at the row index.\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):      \n",
    "        #out.append(y_out[batch_index, 0:column_index].mean(dim=0))  # get the mean vector\n",
    "        out.append(y_out[batch_index, 0:column_index].max(dim=0).values)  # get the max vector\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    \"\"\" an Elman RNN built using the RNNCell \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): size of the input vectors\n",
    "            hidden_size (int): size of the hidden state vectors\n",
    "            bathc_first (bool): whether the 0th dimension is batch\n",
    "        \"\"\"\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        \"\"\"The forward pass of the ElmanRNN\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
    "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
    "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
    "        Returns:\n",
    "            hiddens (torch.Tensor): The outputs of the RNN at each time step. \n",
    "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
    "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "    \n",
    "        hiddens = []\n",
    "\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "\n",
    "        hidden_t = initial_hidden\n",
    "                    \n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)  # x_in[t]: (batch, feat_size); hidden_t: (batch, hidden_size)\n",
    "            hiddens.append(hidden_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
    "                 rnn_hidden_size, bidirectional=False, batch_first=True, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): The size of the word embeddings\n",
    "            num_embeddings (int): The number of words to embed\n",
    "            num_classes (int): The size of the prediction vector \n",
    "                Note: the number of sentiment polarities\n",
    "            bidirectional (bool): Informs whether bidrectional RNN is used\n",
    "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
    "            batch_first (bool): Informs whether the input tensors will \n",
    "                have batch or the sequence on the 0th dimension\n",
    "            padding_idx (int): The index for the tensor padding; \n",
    "                see torch.nn.Embedding\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "\n",
    "        if bidirectional == False:\n",
    "             self.num_directions = 1\n",
    "        else:\n",
    "             self.num_directions = 2\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                embedding_dim=embedding_size,\n",
    "                                padding_idx=padding_idx)\n",
    "        #self.rnn = ElmanRNN(input_size=embedding_size,\n",
    "        #                     hidden_size=rnn_hidden_size,\n",
    "        #                     batch_first=batch_first)\n",
    "\n",
    "#         self.rnn = nn.RNN(input_size=embedding_size,\n",
    "#         self.rnn = nn.GRU(input_size=embedding_size,\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size,\n",
    "                             hidden_size=rnn_hidden_size,\n",
    "                             batch_first=batch_first, \n",
    "                             num_layers = 1,\n",
    "                             dropout = 0.0, \n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(in_features=rnn_hidden_size*self.num_directions,\n",
    "        #                 out_features=rnn_hidden_size*self.num_directions)\n",
    "        self.fc2 = nn.Linear(in_features=rnn_hidden_size*self.num_directions,\n",
    "                          out_features=num_classes)\n",
    "        # for batch norm\n",
    "        self.bn1 = nn.BatchNorm1d(rnn_hidden_size*self.num_directions) \n",
    "\n",
    "\n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
    "                They are used to find the final vector of each sequence\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x_embedded = self.emb(x_in) # x_embedded: (batch, seq_size, feat_size)\n",
    "               \n",
    "        # create PackedSequence; x_packed.data.shape=(number_items, embeddign_size)\n",
    "        #x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(), batch_first=True)\n",
    "        y_out, _ = self.rnn(x_embedded)        \n",
    "        #y_out, _ = pad_packed_sequence(y_out, batch_first=True) # y_out: (batch, seq_size, hidden_size*num_directions)\n",
    "        \n",
    "        if x_lengths is not None:\n",
    "             #y_out = column_gather(y_out, x_lengths)   # y_out gets the last hidden vector of each input: (batch, hidden_size*num_directions)\n",
    "            y_out = column_summation(y_out, x_lengths) # y_out gets the max or mean hidden vector of each input\n",
    "        else:\n",
    "             y_out = y_out[:, -1, :]\n",
    "\n",
    "        \n",
    "        # with batch norm and dropout\n",
    "        #y_out = F.relu(self.bn1(self.fc1(F.dropout(y_out, 0.5, training=self.training))))\n",
    "        #y_out = F.relu(self.bn1(self.fc1(y_out)))\n",
    "\n",
    "        \n",
    "        # with dropout\n",
    "        #y_out = self.fc2(F.dropout(y_out, 0.4, training=self.training))          # y_out: (batch, num_classes)\n",
    "        y_out = self.fc2(y_out)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    review_csv=\"data/Corona_NLP_all_splits.csv\",\n",
    "#     review_csv=\"data/Corona_NLP_all_splits_after_preprocessing.csv\", #after preprocessing data\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/rt-polaritydata/review_classification\",\n",
    "    # Model hyper parameter\n",
    "    word_embedding_size=250,\n",
    "    rnn_hidden_size=256,\n",
    "    bidirectional=False,\n",
    "    #bidirectional=True,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=100,\n",
    "    #num_epochs=200,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    seed=1337,\n",
    "    early_stopping_criteria=10,\n",
    "    # Runtime hyper parameter\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "    # training from a checkpoint\n",
    "    dataset = ReviewDataset.load_dataset_and_load_vectorizer(args.review_csv, \n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(embedding_size=args.word_embedding_size, \n",
    "                               num_embeddings=len(vectorizer.review_vocab),\n",
    "                               num_classes=len(vectorizer.rating_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.review_vocab.mask_index,\n",
    "                               bidirectional=args.bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13812"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.review_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<MASK>': 0,\n",
       " '<UNK>': 1,\n",
       " '<BEGIN>': 2,\n",
       " '<END>': 3,\n",
       " 'consumers': 4,\n",
       " 'eu_consumer': 5,\n",
       " 'and': 6,\n",
       " 'national': 7,\n",
       " 'authorities': 8,\n",
       " 'to': 9,\n",
       " 'tackle': 10,\n",
       " 'online': 11,\n",
       " 'scams': 12,\n",
       " 'fake': 13,\n",
       " 'sales': 14,\n",
       " 'coronavirus': 15,\n",
       " 'covid': 16,\n",
       " 'so': 17,\n",
       " 'awful': 18,\n",
       " 'that': 19,\n",
       " 'people': 20,\n",
       " 'are': 21,\n",
       " 'having': 22,\n",
       " 'self': 23,\n",
       " 'isolate': 24,\n",
       " 'indeed': 25,\n",
       " 'be': 26,\n",
       " 'sick': 27,\n",
       " 'with': 28,\n",
       " 'at': 29,\n",
       " 'home': 30,\n",
       " 'no': 31,\n",
       " 'essentials': 32,\n",
       " 'like': 33,\n",
       " 'loo': 34,\n",
       " 'roll': 35,\n",
       " 'easy': 36,\n",
       " 'cook': 37,\n",
       " 'food': 38,\n",
       " 'for': 39,\n",
       " 'their': 40,\n",
       " 'pets': 41,\n",
       " 'because': 42,\n",
       " 'of': 43,\n",
       " 'selfish': 44,\n",
       " 'stupid': 45,\n",
       " 'panic': 46,\n",
       " 'buyers': 47,\n",
       " 'the': 48,\n",
       " 'supermarkets': 49,\n",
       " 'them': 50,\n",
       " 'did': 51,\n",
       " 'anyone': 52,\n",
       " 'hear': 53,\n",
       " 'in': 54,\n",
       " 'budget': 55,\n",
       " 'a': 56,\n",
       " 'pledge': 57,\n",
       " 'help': 58,\n",
       " 'on': 59,\n",
       " 'banks': 60,\n",
       " 'who': 61,\n",
       " 'now': 62,\n",
       " 'facing': 63,\n",
       " 'as': 64,\n",
       " 'result': 65,\n",
       " 'buying': 66,\n",
       " 'is': 67,\n",
       " 'million': 68,\n",
       " 'severe': 69,\n",
       " 'hardships': 70,\n",
       " 'coming': 71,\n",
       " 'months': 72,\n",
       " 'borisout': 73,\n",
       " 'uk': 74,\n",
       " 'covid_': 75,\n",
       " 'could': 76,\n",
       " 'this': 77,\n",
       " 'crisis': 78,\n",
       " 'tipping': 79,\n",
       " 'point': 80,\n",
       " 'results': 81,\n",
       " '_new': 82,\n",
       " 'normal_': 83,\n",
       " 'cx': 84,\n",
       " 'consumerbehavior': 85,\n",
       " 'cons': 86,\n",
       " 'want': 87,\n",
       " 'focus': 88,\n",
       " 'saving': 89,\n",
       " 'corps': 90,\n",
       " 'amp': 91,\n",
       " 'businesses': 92,\n",
       " 'dems': 93,\n",
       " 'families': 94,\n",
       " 'tell': 95,\n",
       " 'me': 96,\n",
       " 'unless': 97,\n",
       " 'youre': 98,\n",
       " 'talking': 99,\n",
       " 'corporate': 100,\n",
       " 'welfare': 101,\n",
       " 'paid': 102,\n",
       " 'by': 103,\n",
       " 'stealing': 104,\n",
       " 'our': 105,\n",
       " 'taxes': 106,\n",
       " 'how': 107,\n",
       " 'can': 108,\n",
       " 'thrive': 109,\n",
       " 'without': 110,\n",
       " 'healthy': 111,\n",
       " 'financially': 112,\n",
       " 'sound': 113,\n",
       " 'consumer': 114,\n",
       " 'base': 115,\n",
       " 'maga': 116,\n",
       " 'bbc_haveyoursay': 117,\n",
       " 'ive': 118,\n",
       " 'just': 119,\n",
       " 'received': 120,\n",
       " 'letters': 121,\n",
       " 'from': 122,\n",
       " 'bt': 123,\n",
       " 'skyuk': 124,\n",
       " 'raising': 125,\n",
       " 'prices': 126,\n",
       " 'april': 127,\n",
       " 'mad': 128,\n",
       " 'under': 129,\n",
       " 'circumstances': 130,\n",
       " 'youd': 131,\n",
       " 'have': 132,\n",
       " 'thought': 133,\n",
       " 'theyd': 134,\n",
       " 'cancelled': 135,\n",
       " 'it': 136,\n",
       " 'all': 137,\n",
       " 'things': 138,\n",
       " 'considered': 139,\n",
       " 'especially': 140,\n",
       " 'when': 141,\n",
       " 'money': 142,\n",
       " 'tight': 143,\n",
       " 'makes': 144,\n",
       " 'current': 145,\n",
       " 'market': 146,\n",
       " 'volatility': 147,\n",
       " 'worse': 148,\n",
       " 'iea': 149,\n",
       " 'finds': 150,\n",
       " 'we': 151,\n",
       " 'see': 152,\n",
       " 'even': 153,\n",
       " 'more': 154,\n",
       " 'due': 155,\n",
       " 'low': 156,\n",
       " 'epa': 157,\n",
       " 'must': 158,\n",
       " 'stop': 159,\n",
       " 'attacking': 160,\n",
       " 'safeguards': 161,\n",
       " 'new': 162,\n",
       " 'jersey': 163,\n",
       " 'slaps': 164,\n",
       " 'terror': 165,\n",
       " 'charge': 166,\n",
       " 'man': 167,\n",
       " 'over': 168,\n",
       " 'alleged': 169,\n",
       " 'supermarket': 170,\n",
       " 'cough': 171,\n",
       " 'threat': 172,\n",
       " 'total': 173,\n",
       " 'visited': 174,\n",
       " 'shops': 175,\n",
       " 'since': 176,\n",
       " 'pm': 177,\n",
       " 'found': 178,\n",
       " 'gluten': 179,\n",
       " 'free': 180,\n",
       " 'bread': 181,\n",
       " 'mamps': 182,\n",
       " 'plenty': 183,\n",
       " 'aldi': 184,\n",
       " 'pasta': 185,\n",
       " 'or': 186,\n",
       " 'eggs': 187,\n",
       " 'tesco': 188,\n",
       " 'main': 189,\n",
       " 'express': 190,\n",
       " 'sainsburys': 191,\n",
       " 'local': 192,\n",
       " 'polish': 193,\n",
       " 'lidl': 194,\n",
       " 'pissed': 195,\n",
       " 'off': 196,\n",
       " 'lockdownuknow': 197,\n",
       " 'q': 198,\n",
       " 'you': 199,\n",
       " 'spoken': 200,\n",
       " 'about': 201,\n",
       " 'stopping': 202,\n",
       " 'trade': 203,\n",
       " 'war': 204,\n",
       " 'very': 205,\n",
       " 'much': 206,\n",
       " 'helped': 207,\n",
       " 'but': 208,\n",
       " 'producer': 209,\n",
       " 'industry': 210,\n",
       " 'really': 211,\n",
       " 'hurt': 212,\n",
       " 'lot': 213,\n",
       " 'power': 214,\n",
       " 'situation': 215,\n",
       " 'were': 216,\n",
       " 'trying': 217,\n",
       " 'decide': 218,\n",
       " 'what': 219,\n",
       " 'do': 220,\n",
       " 'getting': 221,\n",
       " 'india': 222,\n",
       " 'too': 223,\n",
       " 'lockdown': 224,\n",
       " 'till': 225,\n",
       " 'st': 226,\n",
       " 'running': 227,\n",
       " 'out': 228,\n",
       " 'stock': 229,\n",
       " 'waiting': 230,\n",
       " 'modi': 231,\n",
       " 'speech': 232,\n",
       " 'towards': 233,\n",
       " 'nation': 234,\n",
       " 'today': 235,\n",
       " 'cov': 236,\n",
       " 'msnbc': 237,\n",
       " 'need': 238,\n",
       " 'trillion': 239,\n",
       " 'cover': 240,\n",
       " 'medical': 241,\n",
       " 'debt': 242,\n",
       " 'student': 243,\n",
       " 'except': 244,\n",
       " 'cars': 245,\n",
       " 'mortgages': 246,\n",
       " 'relief': 247,\n",
       " 'shopping': 248,\n",
       " 'my': 249,\n",
       " 'elderly': 250,\n",
       " 'folks': 251,\n",
       " 'morning': 252,\n",
       " 'fascinating': 253,\n",
       " 'observing': 254,\n",
       " 'behaviours': 255,\n",
       " 'place': 256,\n",
       " 'not': 257,\n",
       " 'helpful': 258,\n",
       " 'vulnerable': 259,\n",
       " 'toilet': 260,\n",
       " 'soap': 261,\n",
       " 'left': 262,\n",
       " 'it_s': 263,\n",
       " 'only': 264,\n",
       " 'am': 265,\n",
       " 'mum': 266,\n",
       " 'suffers': 267,\n",
       " 'shes': 268,\n",
       " 'her': 269,\n",
       " 's': 270,\n",
       " 'works': 271,\n",
       " 'large': 272,\n",
       " 'chain': 273,\n",
       " 'government': 274,\n",
       " 'classed': 275,\n",
       " 'risk': 276,\n",
       " 'worried': 277,\n",
       " 'she': 278,\n",
       " 'end': 279,\n",
       " 'up': 280,\n",
       " 'ill': 281,\n",
       " 'if': 282,\n",
       " 'gets': 283,\n",
       " 'cant': 284,\n",
       " 'find': 285,\n",
       " 'anything': 286,\n",
       " 'recommended': 287,\n",
       " 'prove': 288,\n",
       " 'b': 289,\n",
       " 'economic': 290,\n",
       " 'shock': 291,\n",
       " 'which': 292,\n",
       " 'leads': 293,\n",
       " 'deep': 294,\n",
       " 'recession': 295,\n",
       " 'however': 296,\n",
       " 'will': 297,\n",
       " 'last': 298,\n",
       " 'experts': 299,\n",
       " 'predict': 300,\n",
       " 'worst': 301,\n",
       " 'should': 302,\n",
       " 'suggesting': 303,\n",
       " 'possible': 304,\n",
       " 'strong': 305,\n",
       " 'recovery': 306,\n",
       " 'n': 307,\n",
       " 'quickly': 308,\n",
       " 'bouncing': 309,\n",
       " 'back': 310,\n",
       " 'gold': 311,\n",
       " 'suffer': 312,\n",
       " 'sell': 313,\n",
       " 'hits': 314,\n",
       " 'markets': 315,\n",
       " 'ever': 316,\n",
       " 'there': 317,\n",
       " 'was': 318,\n",
       " 'blood': 319,\n",
       " 'streets': 320,\n",
       " 'silver': 321,\n",
       " 'mining': 322,\n",
       " 'investments': 323,\n",
       " 'speculator': 324,\n",
       " 'powell': 325,\n",
       " 'profits': 326,\n",
       " 'oil': 327,\n",
       " 'putin': 328,\n",
       " 'trump': 329,\n",
       " 'stocks': 330,\n",
       " 'china': 331,\n",
       " 'italy': 332,\n",
       " 'your': 333,\n",
       " 'favorite': 334,\n",
       " 'rings': 335,\n",
       " 'include': 336,\n",
       " 'stones': 337,\n",
       " 'handsanitizer': 338,\n",
       " 'shine': 339,\n",
       " 'case': 340,\n",
       " 'damage': 341,\n",
       " 'dont': 342,\n",
       " 'skip': 343,\n",
       " 'sanitizer': 344,\n",
       " 'simply': 345,\n",
       " 'remove': 346,\n",
       " 'before': 347,\n",
       " 'apply': 348,\n",
       " 'washyourhands': 349,\n",
       " 'put': 350,\n",
       " 'epidemic': 351,\n",
       " 'into': 352,\n",
       " 'perspective': 353,\n",
       " 'would': 354,\n",
       " 'rather': 355,\n",
       " 'go': 356,\n",
       " 'through': 357,\n",
       " 'being': 358,\n",
       " 'night': 359,\n",
       " 'after': 360,\n",
       " 'years': 361,\n",
       " 'children': 362,\n",
       " 'couldnt': 363,\n",
       " 'buy': 364,\n",
       " 'commodities': 365,\n",
       " 'strictly': 366,\n",
       " 'rationed': 367,\n",
       " 'id': 368,\n",
       " 'take': 369,\n",
       " 'any': 370,\n",
       " 'day': 371,\n",
       " 'mass': 372,\n",
       " 'then': 373,\n",
       " 'plz': 374,\n",
       " 'post': 375,\n",
       " 'iraq': 376,\n",
       " 'caused': 377,\n",
       " 'corrupt': 378,\n",
       " 'governance': 379,\n",
       " 'pandemic': 380,\n",
       " 'collapse': 381,\n",
       " 'global': 382,\n",
       " 'middle': 383,\n",
       " 'east': 384,\n",
       " 'corruption': 385,\n",
       " 'corona': 386,\n",
       " 'virus': 387,\n",
       " 'iraqi': 388,\n",
       " 'survive': 389,\n",
       " 'demise': 390,\n",
       " 'green': 391,\n",
       " 'zone': 392,\n",
       " 'el': 393,\n",
       " 'bank': 394,\n",
       " 'queensland': 395,\n",
       " 'boss': 396,\n",
       " 'george': 397,\n",
       " 'warns': 398,\n",
       " 'steep': 399,\n",
       " 'fall': 400,\n",
       " 'housing': 401,\n",
       " 'bites': 402,\n",
       " 'margins': 403,\n",
       " 'some': 404,\n",
       " 'cases': 405,\n",
       " 'negative': 406,\n",
       " 'equity': 407,\n",
       " 'thinks': 408,\n",
       " 'gov': 409,\n",
       " 't': 410,\n",
       " 'provide': 411,\n",
       " 'emergency': 412,\n",
       " 'stimulus': 413,\n",
       " 'jobless': 414,\n",
       " 'rate': 415,\n",
       " 'goes': 416,\n",
       " 'double': 417,\n",
       " 'digits': 418,\n",
       " 'us': 419,\n",
       " 'crude': 420,\n",
       " 'hit': 421,\n",
       " 'year': 422,\n",
       " 'looms': 423,\n",
       " 'economy': 424,\n",
       " 'read': 425,\n",
       " 'updates': 426,\n",
       " 'here': 427,\n",
       " 'becoming': 428,\n",
       " 'social': 429,\n",
       " 'unrest': 430,\n",
       " 'brewing': 431,\n",
       " 'police': 432,\n",
       " 'descend': 433,\n",
       " 'reports': 434,\n",
       " 'stolen': 435,\n",
       " 'feed': 436,\n",
       " 'themselves': 437,\n",
       " 'patience': 438,\n",
       " 'turns': 439,\n",
       " 'desperation': 440,\n",
       " 'immediate': 441,\n",
       " 'aftermath': 442,\n",
       " 'expected': 443,\n",
       " 'feature': 444,\n",
       " 'high': 445,\n",
       " 'unemployment': 446,\n",
       " 'spending': 447,\n",
       " 'depleted': 448,\n",
       " 'savings': 449,\n",
       " 'slow': 450,\n",
       " 'international': 451,\n",
       " 'travel': 452,\n",
       " 'business': 453,\n",
       " 'activity': 454,\n",
       " 'covidug': 455,\n",
       " 'staysafestayhome': 456,\n",
       " 'preying': 457,\n",
       " 'fears': 458,\n",
       " 'spreading': 459,\n",
       " 'fast': 460,\n",
       " 'disease': 461,\n",
       " 'itself': 462,\n",
       " 'scammers': 463,\n",
       " 'exploit': 464,\n",
       " 'public': 465,\n",
       " 'own': 466,\n",
       " 'gain': 467,\n",
       " 'we_ve': 468,\n",
       " 'got': 469,\n",
       " 'info': 470,\n",
       " 'unfortunately': 471,\n",
       " 'outbreak': 472,\n",
       " 'continues': 473,\n",
       " 'spread': 474,\n",
       " 'fraudsters': 475,\n",
       " 'may': 476,\n",
       " 'try': 477,\n",
       " 'profit': 478,\n",
       " 'canada': 479,\n",
       " 'has': 480,\n",
       " 'list': 481,\n",
       " 'potential': 482,\n",
       " 'tips': 483,\n",
       " 'protect': 484,\n",
       " 'yourself': 485,\n",
       " 'philippines': 486,\n",
       " 'hoarding': 487,\n",
       " 'supplies': 488,\n",
       " 'smaller': 489,\n",
       " 'selling': 490,\n",
       " 'overly': 491,\n",
       " 'dear': 492,\n",
       " 'fellow': 493,\n",
       " 'filipinos': 494,\n",
       " 'shame': 495,\n",
       " 'rest': 496,\n",
       " 'waste': 497,\n",
       " 'space': 498,\n",
       " 'earth': 499,\n",
       " 'nothing': 500,\n",
       " 'doing': 501,\n",
       " 'great': 502,\n",
       " 'quarantined': 503,\n",
       " 'dying': 504,\n",
       " 'nomoney': 505,\n",
       " 'nofood': 506,\n",
       " 'sickening': 507,\n",
       " 'idiot': 508,\n",
       " 'well': 509,\n",
       " 'done': 510,\n",
       " 'sainsbury': 511,\n",
       " 'keynes': 512,\n",
       " 'telling': 513,\n",
       " 'customers': 514,\n",
       " 'one': 515,\n",
       " 'item': 516,\n",
       " 'each': 517,\n",
       " 'product': 518,\n",
       " 'range': 519,\n",
       " 'store': 520,\n",
       " 'relaxed': 521,\n",
       " 'less': 522,\n",
       " 'stress': 523,\n",
       " 'bad': 524,\n",
       " 'behaviour': 525,\n",
       " 'across': 526,\n",
       " 'retail': 527,\n",
       " 'sector': 528,\n",
       " 'imrankhanpti': 529,\n",
       " 'shot': 530,\n",
       " 'everything': 531,\n",
       " 'inflation': 532,\n",
       " 'didnt': 533,\n",
       " 'its': 534,\n",
       " 'insanity': 535,\n",
       " 'device': 536,\n",
       " 'regulate': 537,\n",
       " 'prison': 538,\n",
       " 'horders': 539,\n",
       " 'those': 540,\n",
       " 'cashing': 541,\n",
       " 'humanity': 542,\n",
       " 'dead': 543,\n",
       " 'pakistan': 544,\n",
       " 'walmart': 545,\n",
       " 'needs': 546,\n",
       " 'limit': 547,\n",
       " 'paper': 548,\n",
       " 'per': 549,\n",
       " 'customer': 550,\n",
       " 'went': 551,\n",
       " 'basic': 552,\n",
       " 'witness': 553,\n",
       " 'savages': 554,\n",
       " 'fighting': 555,\n",
       " 'had': 556,\n",
       " 'carts': 557,\n",
       " 'full': 558,\n",
       " 'items': 559,\n",
       " 'making': 560,\n",
       " 'personal': 561,\n",
       " 'rice': 562,\n",
       " 'near': 563,\n",
       " 'ask': 564,\n",
       " 'why': 565,\n",
       " 'they': 566,\n",
       " 'said': 567,\n",
       " 'brought': 568,\n",
       " 'kilos': 569,\n",
       " 'fear': 570,\n",
       " 'seriously': 571,\n",
       " 'still': 572,\n",
       " 'yall': 573,\n",
       " 'act': 574,\n",
       " 'infected': 575,\n",
       " 'best': 576,\n",
       " 'i': 577,\n",
       " 'believe': 578,\n",
       " 'say': 579,\n",
       " 'bananas': 580,\n",
       " 'cure': 581,\n",
       " 'banana': 582,\n",
       " 'aisle': 583,\n",
       " 'completely': 584,\n",
       " 'decimated': 585,\n",
       " 'sure': 586,\n",
       " 'misleading': 587,\n",
       " 'video': 588,\n",
       " 'thats': 589,\n",
       " 'been': 590,\n",
       " 'floating': 591,\n",
       " 'around': 592,\n",
       " 'fb': 593,\n",
       " 'fucking': 594,\n",
       " 'liar': 595,\n",
       " 'helping': 596,\n",
       " 'does': 597,\n",
       " 'working': 598,\n",
       " 'class': 599,\n",
       " 'leave': 600,\n",
       " 'testing': 601,\n",
       " 'treatment': 602,\n",
       " 'payments': 603,\n",
       " 'expanding': 604,\n",
       " 'than': 605,\n",
       " 'reducing': 606,\n",
       " 'expect': 607,\n",
       " 'an': 608,\n",
       " 'gulf': 609,\n",
       " 'arab': 610,\n",
       " 'states': 611,\n",
       " 'lack': 612,\n",
       " 'fiscal': 613,\n",
       " 'watch': 614,\n",
       " 'live': 615,\n",
       " 'via': 616,\n",
       " 'zoom': 617,\n",
       " 'link': 618,\n",
       " 'below': 619,\n",
       " 'gotta': 620,\n",
       " 'run': 621,\n",
       " 'grocery': 622,\n",
       " 'stay': 623,\n",
       " 'hell': 624,\n",
       " 'away': 625,\n",
       " 'germs': 626,\n",
       " 'thanks': 627,\n",
       " 'state': 628,\n",
       " 'socialdistancing': 629,\n",
       " 'queue': 630,\n",
       " 'enter': 631,\n",
       " 'ordered': 632,\n",
       " 'somber': 633,\n",
       " 'meter': 634,\n",
       " 'apart': 635,\n",
       " 'directed': 636,\n",
       " 'fully': 637,\n",
       " 'stocked': 638,\n",
       " 'taking': 639,\n",
       " 'coronaoutbreak': 640,\n",
       " 'coronavirusupdate': 641,\n",
       " 'coronavirusitalia': 642,\n",
       " 'homedepot': 643,\n",
       " 'horrible': 644,\n",
       " 'job': 645,\n",
       " 'absolute': 646,\n",
       " 'chaos': 647,\n",
       " 'curbside': 648,\n",
       " 'pickup': 649,\n",
       " 'never': 650,\n",
       " 'mind': 651,\n",
       " 'scores': 652,\n",
       " 'walking': 653,\n",
       " 'wonder': 654,\n",
       " 'keeps': 655,\n",
       " 'dumb': 656,\n",
       " 'robbing': 657,\n",
       " 'masks': 658,\n",
       " 'ridiculous': 659,\n",
       " 'filters': 660,\n",
       " 'give': 661,\n",
       " 'seller': 662,\n",
       " 'feedback': 663,\n",
       " 'profiteering': 664,\n",
       " 'suffering': 665,\n",
       " 'death': 666,\n",
       " 'unacceptable': 667,\n",
       " 'rare': 668,\n",
       " 'right': 669,\n",
       " 'letting': 670,\n",
       " 'gilead': 671,\n",
       " 'complete': 672,\n",
       " 'control': 673,\n",
       " 'drugs': 674,\n",
       " 'mean': 675,\n",
       " 'set': 676,\n",
       " 'supply': 677,\n",
       " 'call': 678,\n",
       " 'representatives': 679,\n",
       " 'carers': 680,\n",
       " 'delivery': 681,\n",
       " 'drivers': 682,\n",
       " 'look': 683,\n",
       " 'signs': 684,\n",
       " 'domestic': 685,\n",
       " 'abuse': 686,\n",
       " 'until': 687,\n",
       " 'globe': 688,\n",
       " 'sadly': 689,\n",
       " 'continue': 690,\n",
       " 'city': 691,\n",
       " 'don_t': 692,\n",
       " 'urgency': 693,\n",
       " 'i_m': 694,\n",
       " 'difficult': 695,\n",
       " 'hard': 696,\n",
       " 'week': 697,\n",
       " 'propylene': 698,\n",
       " 'witnessed': 699,\n",
       " 'drop': 700,\n",
       " 'asia': 701,\n",
       " 'price': 702,\n",
       " 'tumbled': 703,\n",
       " 'momentum': 704,\n",
       " 'extremely': 705,\n",
       " 'sluggish': 706,\n",
       " 'owing': 707,\n",
       " 'major': 708,\n",
       " 'asian': 709,\n",
       " 'countries': 710,\n",
       " 'widespread': 711,\n",
       " 'click': 712,\n",
       " 'ongoing': 713,\n",
       " 'weakened': 714,\n",
       " 'driving': 715,\n",
       " 'down': 716,\n",
       " 'demand': 717,\n",
       " 'increases': 718,\n",
       " 'during': 719,\n",
       " 'services': 720,\n",
       " 'west': 721,\n",
       " 'michigan': 722,\n",
       " 'launches': 723,\n",
       " 'meal': 724,\n",
       " 'pick': 725,\n",
       " 'ups': 726,\n",
       " 'mombasa': 727,\n",
       " 'crime': 728,\n",
       " 'alerts': 729,\n",
       " 'revoke': 730,\n",
       " 'licenses': 731,\n",
       " 'traders': 732,\n",
       " 'hiked': 733,\n",
       " 'liquefied': 734,\n",
       " 'petroleum': 735,\n",
       " 'gas': 736,\n",
       " 'lpg': 737,\n",
       " 'kenya': 738,\n",
       " 'battles': 739,\n",
       " 'where': 740,\n",
       " 'fuck': 741,\n",
       " 'these': 742,\n",
       " 'type': 743,\n",
       " 'bought': 744,\n",
       " 'though': 745,\n",
       " 'happens': 746,\n",
       " 'get': 747,\n",
       " 'mom': 748,\n",
       " 'die': 749,\n",
       " 'correct': 750,\n",
       " 'going': 751,\n",
       " 'true': 752,\n",
       " 'think': 753,\n",
       " 'make': 754,\n",
       " 'weeks': 755,\n",
       " 'ago': 756,\n",
       " 'diarrhoea': 757,\n",
       " 'purchased': 758,\n",
       " 'pack': 759,\n",
       " 'tp': 760,\n",
       " 'inconsiderate': 761,\n",
       " 'arsehole': 762,\n",
       " 'regard': 763,\n",
       " 'tpshortage': 764,\n",
       " 'heartbreaking': 765,\n",
       " 'devastating': 766,\n",
       " 'covidwalkout': 767,\n",
       " 'coronavirususa': 768,\n",
       " 'gok': 769,\n",
       " 'immediately': 770,\n",
       " 'suspend': 771,\n",
       " 'vat': 772,\n",
       " 'excise': 773,\n",
       " 'duties': 774,\n",
       " 'fees': 775,\n",
       " 'goods': 776,\n",
       " 'least': 777,\n",
       " 'thru': 778,\n",
       " 'thereby': 779,\n",
       " 'instantly': 780,\n",
       " 'entire': 781,\n",
       " 'benefiting': 782,\n",
       " 'everyone': 783,\n",
       " 'such': 784,\n",
       " 'action': 785,\n",
       " 'harsh': 786,\n",
       " 'effects': 787,\n",
       " 'work': 788,\n",
       " 'anticipating': 789,\n",
       " 'shit': 790,\n",
       " 'storm': 791,\n",
       " 'news': 792,\n",
       " 'highlights': 793,\n",
       " 'fucked': 794,\n",
       " 'shits': 795,\n",
       " 'crazy': 796,\n",
       " 'dumping': 797,\n",
       " 'milk': 798,\n",
       " 'crushed': 799,\n",
       " 'schools': 800,\n",
       " 'processors': 801,\n",
       " 'same': 802,\n",
       " 'time': 803,\n",
       " 'unexpected': 804,\n",
       " 'surge': 805,\n",
       " 'born': 806,\n",
       " 'govt': 807,\n",
       " 'few': 808,\n",
       " 'tools': 809,\n",
       " 'other': 810,\n",
       " 'excess': 811,\n",
       " 'sounds': 812,\n",
       " 'familiar': 813,\n",
       " 'shut': 814,\n",
       " 'cow': 815,\n",
       " 'plunge': 816,\n",
       " 'futures': 817,\n",
       " 'hitting': 818,\n",
       " 'governments': 819,\n",
       " 'worldwide': 820,\n",
       " 'accelerated': 821,\n",
       " 'lockdowns': 822,\n",
       " 'counter': 823,\n",
       " 'causing': 824,\n",
       " 'fuel': 825,\n",
       " 'bill': 826,\n",
       " 'gates': 827,\n",
       " 'here_s': 828,\n",
       " 'lost': 829,\n",
       " 'washington': 830,\n",
       " 'governors': 831,\n",
       " 'compete': 832,\n",
       " 'lifesaving': 833,\n",
       " 'equipment': 834,\n",
       " 'hospitals': 835,\n",
       " 'pay': 836,\n",
       " 'exorbitant': 837,\n",
       " 'matters': 838,\n",
       " 'hong': 839,\n",
       " 'kong': 840,\n",
       " 'singapore': 841,\n",
       " 'two': 842,\n",
       " 'cities': 843,\n",
       " 'big': 844,\n",
       " 'revenue': 845,\n",
       " 'leisure': 846,\n",
       " 'activities': 847,\n",
       " 'restaurants': 848,\n",
       " 'accommodation': 849,\n",
       " 'plummets': 850,\n",
       " 'weaker': 851,\n",
       " 'drive': 852,\n",
       " 'cheaper': 853,\n",
       " 'inhabitants': 854,\n",
       " 'bet': 855,\n",
       " 'tax': 856,\n",
       " 'blame': 857,\n",
       " 'loss': 858,\n",
       " 'wages': 859,\n",
       " 'justsaying': 860,\n",
       " 'technology': 861,\n",
       " 'short': 862,\n",
       " 'resources': 863,\n",
       " 'sufficient': 864,\n",
       " 'handle': 865,\n",
       " 'challenges': 866,\n",
       " 'inventory': 867,\n",
       " 'napkins': 868,\n",
       " 'towel': 869,\n",
       " 'gouging': 870,\n",
       " 'complaints': 871,\n",
       " 'spike': 872,\n",
       " 'amid': 873,\n",
       " 'transparent': 874,\n",
       " 'effective': 875,\n",
       " 'several': 876,\n",
       " 'worth': 877,\n",
       " 'stockpile': 878,\n",
       " 'medications': 879,\n",
       " 'cause': 880,\n",
       " 'cold': 881,\n",
       " 'unclear': 882,\n",
       " 'lines': 883,\n",
       " 'affected': 884,\n",
       " 'workers': 885,\n",
       " 'deserve': 886,\n",
       " 'hr': 887,\n",
       " 'also': 888,\n",
       " 'omg': 889,\n",
       " 'part': 890,\n",
       " 'wants': 891,\n",
       " 'reckless': 892,\n",
       " 'dye': 893,\n",
       " 'hair': 894,\n",
       " 'purple': 895,\n",
       " 'nintendo': 896,\n",
       " 'switch': 897,\n",
       " 'age': 898,\n",
       " 'pure': 899,\n",
       " 'utter': 900,\n",
       " 'love': 901,\n",
       " 'send': 902,\n",
       " 'seeing': 903,\n",
       " 'tweets': 904,\n",
       " 'suggest': 905,\n",
       " 'trial': 906,\n",
       " 'socialism': 907,\n",
       " 'whats': 908,\n",
       " 'hiking': 909,\n",
       " 'moronic': 910,\n",
       " 'shows': 911,\n",
       " 'knowledge': 912,\n",
       " 'capitalism': 913,\n",
       " 'arent': 914,\n",
       " 'owned': 915,\n",
       " 'la': 916,\n",
       " 'council': 917,\n",
       " 'key': 918,\n",
       " 'introduced': 919,\n",
       " 'motion': 920,\n",
       " 'calling': 921,\n",
       " 'blanket': 922,\n",
       " 'ban': 923,\n",
       " 'evictions': 924,\n",
       " 'rent': 925,\n",
       " 'deferred': 926,\n",
       " 'treated': 927,\n",
       " 'avoid': 928,\n",
       " 'based': 929,\n",
       " 'coded': 930,\n",
       " 'arms': 931,\n",
       " 'mother': 932,\n",
       " 'recalls': 933,\n",
       " 'yearold': 934,\n",
       " 'daughters': 935,\n",
       " 'moments': 936,\n",
       " 'sad': 937,\n",
       " 'lose': 938,\n",
       " 'child': 939,\n",
       " 'heart': 940,\n",
       " 'hurts': 941,\n",
       " 'family': 942,\n",
       " 'held': 943,\n",
       " 'responsible': 944,\n",
       " 'he': 945,\n",
       " 'told': 946,\n",
       " 'airtelnigeria': 947,\n",
       " 'fight': 948,\n",
       " 'data': 949,\n",
       " 'managers': 950,\n",
       " 'splitting': 951,\n",
       " 'packages': 952,\n",
       " 'reduce': 953,\n",
       " 'pains': 954,\n",
       " 'selfisolation': 955,\n",
       " 'handwashing': 956,\n",
       " 'good': 957,\n",
       " 'inadequate': 958,\n",
       " 'germ': 959,\n",
       " 'phone': 960,\n",
       " 'touches': 961,\n",
       " 'wipe': 962,\n",
       " 'w': 963,\n",
       " 'alcohol': 964,\n",
       " 'swab': 965,\n",
       " 'rub': 966,\n",
       " 'hands': 967,\n",
       " 'sec': 968,\n",
       " 'til': 969,\n",
       " 'dry': 970,\n",
       " 'pump': 971,\n",
       " 'dispensers': 972,\n",
       " 'saudi': 973,\n",
       " 'arabia': 974,\n",
       " 'russia': 975,\n",
       " 'delay': 976,\n",
       " 'meeting': 977,\n",
       " 'discuss': 978,\n",
       " 'output': 979,\n",
       " 'cuts': 980,\n",
       " 'curb': 981,\n",
       " 'oversupply': 982,\n",
       " 'face': 983,\n",
       " 'plummeting': 984,\n",
       " 'oott': 985,\n",
       " 'someone': 986,\n",
       " 'category': 987,\n",
       " 'im': 988,\n",
       " 'currently': 989,\n",
       " 'concerned': 990,\n",
       " 'unable': 991,\n",
       " 'access': 992,\n",
       " 'catching': 993,\n",
       " 'actual': 994,\n",
       " 'na': 995,\n",
       " 'da': 996,\n",
       " 'des': 997,\n",
       " 'ist': 998,\n",
       " 'elizabeth': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.review_vocab._token_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "    \n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30e636d915143acb2292122f2500d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a79bdadfa1b42a293f1694da4e509fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91f0089790c47c8900474e6eb52a52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "           \n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'], \n",
    "                                x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    \n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'], \n",
    "                                x_lengths=batch_dict['x_length'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = train_state['train_acc']\n",
    "val_acc = train_state['val_acc']\n",
    "loss = train_state['train_loss']\n",
    "val_loss = train_state['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "y_pred_list = []         # store predicted values for confusion matrix\n",
    "y_rating_list = []  # ground truth value\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'],\n",
    "                         x_lengths=batch_dict['x_length'])\n",
    "\n",
    "    # store predicted values and ground truth values for calculating confusion matrix\n",
    "    y_pred_list.extend(y_pred.max(dim=1)[1].cpu().numpy())\n",
    "    y_rating_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7423358010172281;\n",
      "Test Accuracy: 71.22122545700744\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "rating_classes = []\n",
    "for i in range(len(dataset._vectorizer.rating_vocab)):\n",
    "    rating_classes.append(dataset._vectorizer.rating_vocab.lookup_index(i))\n",
    "print(rating_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True                Extremely Negative  Extremely Positive  Negative  Neutral  \\\n",
      "Predicted                                                                       \n",
      "Extremely Negative                 692                   3       249        9   \n",
      "Extremely Positive                   4                 824        16        7   \n",
      "Negative                           190                  19      1054      131   \n",
      "Neutral                              7                   6       158      988   \n",
      "Positive                            19                 232       168      116   \n",
      "\n",
      "True                Positive  \n",
      "Predicted                     \n",
      "Extremely Negative        20  \n",
      "Extremely Positive       233  \n",
      "Negative                 246  \n",
      "Neutral                  108  \n",
      "Positive                1249  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cm = confusion_matrix(y_rating_list, y_pred_list)\n",
    "cm_df = pd.DataFrame(cm.T, index=rating_classes, columns=rating_classes)\n",
    "cm_df.index.name = 'Predicted'\n",
    "cm_df.columns.name = 'True'\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73       912\n",
      "           1       0.76      0.76      0.76      1084\n",
      "           2       0.64      0.64      0.64      1645\n",
      "           3       0.78      0.79      0.78      1251\n",
      "           4       0.70      0.67      0.69      1856\n",
      "\n",
      "    accuracy                           0.71      6748\n",
      "   macro avg       0.72      0.72      0.72      6748\n",
      "weighted avg       0.71      0.71      0.71      6748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_rating_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, classifier, vectorizer):\n",
    "    vectorized_review, vec_length = vectorizer.vectorize(review)\n",
    "    vectorized_review = torch.tensor(vectorized_review).unsqueeze(dim=0)\n",
    "    vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
    "    \n",
    "    result = classifier(vectorized_review, vec_length, apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    \n",
    "    index = indices.item()\n",
    "    prob_value = probability_values.item()\n",
    "\n",
    "    predicted_rating = vectorizer.rating_vocab.lookup_index(index)\n",
    "\n",
    "    return {'sentiment': predicted_rating, 'probability': prob_value, 'review': review}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Negative', 'probability': 0.5352278351783752, 'review': 'I hate Covid'}\n",
      "{'sentiment': 'Extremely Negative', 'probability': 0.881805419921875, 'review': 'I hate Covid since it is so bad'}\n",
      "{'sentiment': 'Negative', 'probability': 0.6968895792961121, 'review': 'I hate Text & Web Mining Class'}\n",
      "{'sentiment': 'Positive', 'probability': 0.9053123593330383, 'review': 'I like Covid, as I can stay at home'}\n",
      "{'sentiment': 'Positive', 'probability': 0.7884840965270996, 'review': 'I like Covid so much, since I can stay at home'}\n"
     ]
    }
   ],
   "source": [
    "# review = input(\"Enter a review: \")\n",
    "review1 = \"I hate Covid\"\n",
    "review2 = \"I hate Covid since it is so bad\"\n",
    "review3 = \"I hate Text & Web Mining Class\"\n",
    "review4 = \"I like Covid, as I can stay at home\"\n",
    "review5 = \"I like Covid so much, since I can stay at home\"\n",
    "classifier = classifier.to(\"cpu\")\n",
    "print(predict_sentiment(review1, classifier, vectorizer))\n",
    "print(predict_sentiment(review2, classifier, vectorizer))\n",
    "print(predict_sentiment(review3, classifier, vectorizer))\n",
    "print(predict_sentiment(review4, classifier, vectorizer))\n",
    "print(predict_sentiment(review5, classifier, vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
